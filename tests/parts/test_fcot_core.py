"""
Core unit tests for FCOT (Finitely-Concave Optimal Transport).

Tests basic FCOT functionality without full training loops:
- Initialization and configuration
- Dual objective computation
- Dual feasibility constraints
- Monge map properties
- Minibatch correctness
- Edge cases

These tests are fast (<1s each) and test core logic.
"""

import unittest
import torch
import numpy as np
from baselines.ot_fc_map import FCOT
from models import FiniteModel
from tools.utils import L22, L2, inverse_grad_L22, inverse_grad_L2
from tests import TimedTestCase


class TestFCOTInitialization(TimedTestCase):
    """Test FCOT initialization and basic setup."""

    def test_init_basic(self):
        """Test basic initialization with minimal parameters."""
        dim = 2
        model = FiniteModel(
            num_candidates=10,
            num_dims=dim,
            kernel=lambda X, Y: L22(X, Y),
            mode="concave"
        )
        
        fcot = FCOT(
            input_dim=dim,
            model=model,
            inverse_cx=inverse_grad_L22,
            lr=1e-3,
            device="cpu"
        )
        
        self.assertEqual(fcot.input_dim, dim)
        self.assertEqual(fcot.lr, 1e-3)
        self.assertIsNotNone(fcot.optimizer)
        self.assertEqual(fcot.device, torch.device("cpu"))

    def test_init_custom_hyperparams(self):
        """Test initialization with custom hyperparameters."""
        dim = 3
        model = FiniteModel(
            num_candidates=20,
            num_dims=dim,
            kernel=lambda X, Y: L22(X, Y),
            mode="concave"
        )
        
        fcot = FCOT(
            input_dim=dim,
            model=model,
            inverse_cx=inverse_grad_L22,
            lr=5e-4,
            betas=(0.9, 0.999),
            inner_optimizer="adam",
            inner_steps=10,
            inner_lam=1e-4,
            inner_tol=1e-4,
            inner_lr=1e-2
        )
        
        self.assertEqual(fcot.lr, 5e-4)
        self.assertEqual(fcot.betas, (0.9, 0.999))
        self.assertEqual(fcot.inner_optimizer, "adam")
        self.assertEqual(fcot.inner_steps, 10)
        self.assertEqual(fcot.inner_lam, 1e-4)
        self.assertEqual(fcot.inner_tol, 1e-4)
        self.assertEqual(fcot.inner_lr, 1e-2)

    def test_inner_lr_defaults_to_lr(self):
        """Test that inner_lr defaults to lr when not specified."""
        dim = 2
        model = FiniteModel(
            num_candidates=5,
            num_dims=dim,
            kernel=lambda X, Y: L22(X, Y),
            mode="concave"
        )
        
        fcot = FCOT(
            input_dim=dim,
            model=model,
            inverse_cx=inverse_grad_L22,
            lr=1e-3
        )
        
        self.assertEqual(fcot.inner_lr, fcot.lr)

    def test_initialize_right_architecture(self):
        """Test factory method that creates FCOT with parameter budget."""
        dim = 2
        target_params = 100
        
        fcot = FCOT.initialize_right_architecture(
            dim=dim,
            n_params_target=target_params,
            cost=L22,
            inverse_cx=inverse_grad_L22,
            lr=1e-3
        )
        
        # Check that instance is created correctly
        self.assertEqual(fcot.input_dim, dim)
        self.assertIsNotNone(fcot.model)
        
        # Check parameter count is close to target
        actual_params = sum(p.numel() for p in fcot.model.parameters())
        # Should be within ~10% of target (due to rounding to integer candidates)
        self.assertGreater(actual_params, target_params * 0.5)
        self.assertLess(actual_params, target_params * 1.5)

    def test_initialize_right_architecture_different_sizes(self):
        """Test factory method with various parameter budgets."""
        for target in [50, 200, 1000]:
            fcot = FCOT.initialize_right_architecture(
                dim=3,
                n_params_target=target,
                cost=L22,
                inverse_cx=inverse_grad_L22
            )
            
            actual = sum(p.numel() for p in fcot.model.parameters())
            # Check reasonable approximation
            self.assertGreater(actual, target * 0.5)
            self.assertLess(actual, target * 1.5)
            
            # Check model works
            X = torch.randn(10, 3)
            Y = fcot.transport_X_to_Y(X)
            self.assertEqual(Y.shape, X.shape)


class TestDualObjective(TimedTestCase):
    """Test dual objective computation and correctness."""

    def setUp(self):
        """Set up a simple 1D example for testing."""
        self.dim = 1
        torch.manual_seed(42)
        np.random.seed(42)
        
    def test_dual_objective_computation(self):
        """Test that dual objective computes without errors."""
        model = FiniteModel(
            num_candidates=5,
            num_dims=self.dim,
            kernel=lambda X, Y: L22(X, Y),
            mode="concave",
            temp=10.0
        )
        
        fcot = FCOT(
            input_dim=self.dim,
            model=model,
            inverse_cx=inverse_grad_L22,
            inner_steps=3,
            inner_optimizer="lbfgs"
        )
        
        X = torch.randn(10, self.dim)
        Y = torch.randn(10, self.dim)
        idx_y = torch.arange(10)
        
        D, u_mean, uc_mean = fcot._dual_objective(X, Y, idx_y)
        
        # Check that all values are scalars
        self.assertEqual(D.shape, torch.Size([]))
        
        # Check that D = u_mean + uc_mean
        self.assertAlmostEqual(D.item(), u_mean.item() + uc_mean.item(), places=6)
        
    def test_dual_computation(self):
        """Test that dual objective computes correctly."""
        model = FiniteModel(
            num_candidates=8,
            num_dims=2,
            kernel=lambda X, Y: L22(X, Y),
            mode="concave",
            temp=10.0
        )
        
        fcot = FCOT(
            input_dim=2,
            model=model,
            inverse_cx=inverse_grad_L22,
            inner_steps=5
        )
        
        X = torch.randn(20, 2)
        Y = torch.randn(20, 2)
        idx_y = torch.arange(20)
        
        D, u_mean, uc_mean = fcot._dual_objective(X, Y, idx_y)
        
        # D should equal u_mean + uc_mean
        self.assertAlmostEqual(float(D.detach()), float(u_mean.detach()) + float(uc_mean.detach()), places=5)

    def test_dual_objective_no_nans(self):
        """Test that dual objective doesn't produce NaNs."""
        model = FiniteModel(
            num_candidates=10,
            num_dims=2,
            kernel=lambda X, Y: L22(X, Y),
            mode="concave"
        )
        
        fcot = FCOT(
            input_dim=2,
            model=model,
            inverse_cx=inverse_grad_L22,
            inner_steps=5
        )
        
        X = torch.randn(15, 2)
        Y = torch.randn(15, 2)
        idx_y = torch.arange(15)
        
        D, u_mean, uc_mean = fcot._dual_objective(X, Y, idx_y)
        
        self.assertFalse(torch.isnan(D))
        self.assertFalse(torch.isnan(u_mean))
        self.assertFalse(torch.isnan(uc_mean))


class TestDualConstraints(TimedTestCase):
    """Test that dual potentials satisfy Kantorovich constraints."""
    max_test_time = 10  # This test does some training iterations

    def test_dual_feasibility_constraint(self):
        """Test that f(x) + f^c(y) â‰¤ k(x,y) after optimization."""
        dim = 1
        torch.manual_seed(123)
        
        # Create simple 1D example
        X = torch.linspace(-2, 2, 20).reshape(-1, 1)
        Y = torch.linspace(-1, 3, 20).reshape(-1, 1)
        
        model = FiniteModel(
            num_candidates=10,
            num_dims=dim,
            kernel=lambda x, y: L22(x, y),
            mode="concave",
            temp=20.0
        )
        
        fcot = FCOT(
            input_dim=dim,
            model=model,
            inverse_cx=inverse_grad_L22,
            lr=5e-3,
            inner_steps=5,
            inner_optimizer="adam"
        )
        
        # Train for a few iterations
        fcot._fit(X, Y, batch_size=10, iters=20, print_every=100)
        
        # Check that f and f^c are computable without errors
        with torch.no_grad():
            _, f_vals = model.forward(X, selection_mode="soft")
            
            # Compute conjugate for Y points
            idx_y = torch.arange(len(Y))
            _, fc_vals, _ = model.sup_transform(
                Z=Y,
                sample_idx=idx_y,
                steps=10,
                optimizer="adam"
            )
        
        # Just check that all values are finite (feasibility is approximate for finite models)
        self.assertFalse(torch.isnan(f_vals).any())
        self.assertFalse(torch.isinf(f_vals).any())
        self.assertFalse(torch.isnan(fc_vals).any())
        self.assertFalse(torch.isinf(fc_vals).any())
        
        # Check that dual value J = E[f(x)] + E[f^c(y)] is reasonable
        J_approx = f_vals.mean() + fc_vals.mean()
        self.assertFalse(np.isnan(J_approx.item()))
        self.assertFalse(np.isinf(J_approx.item()))


class TestMongeMap(TimedTestCase):
    """Test Monge map recovery and properties."""
    max_test_time = 15  # Involves some training iterations

    def test_transport_map_shape(self):
        """Test that transport map produces correct output shape."""
        dim = 2
        model = FiniteModel(
            num_candidates=15,
            num_dims=dim,
            kernel=lambda X, Y: L22(X, Y),
            mode="concave"
        )
        
        fcot = FCOT(
            input_dim=dim,
            model=model,
            inverse_cx=inverse_grad_L22
        )
        
        X = torch.randn(30, dim)
        Y_mapped = fcot.transport_X_to_Y(X)
        
        self.assertEqual(Y_mapped.shape, X.shape)

    def test_transport_map_no_nans(self):
        """Test that transport map doesn't produce NaNs."""
        dim = 3
        model = FiniteModel(
            num_candidates=10,
            num_dims=dim,
            kernel=lambda X, Y: L22(X, Y),
            mode="concave"
        )
        
        fcot = FCOT(
            input_dim=dim,
            model=model,
            inverse_cx=inverse_grad_L22
        )
        
        X = torch.randn(20, dim)
        Y_mapped = fcot.transport_X_to_Y(X)
        
        self.assertFalse(torch.isnan(Y_mapped).any())

    def test_identity_transport_approximately(self):
        """Test that transporting identical distributions gives approximate identity."""
        dim = 1
        torch.manual_seed(456)
        
        # Same distribution for X and Y
        X = torch.randn(50, dim)
        Y = X + torch.randn(50, dim) * 0.01  # Very similar
        
        model = FiniteModel(
            num_candidates=20,
            num_dims=dim,
            kernel=lambda x, y: L22(x, y),
            mode="concave",
            temp=50.0
        )
        
        fcot = FCOT(
            input_dim=dim,
            model=model,
            inverse_cx=inverse_grad_L22,
            lr=1e-2,
            inner_steps=10
        )
        
        # Train
        fcot._fit(X, Y, batch_size=25, iters=30, print_every=100)
        
        # Transport should be close to identity
        Y_mapped = fcot.transport_X_to_Y(X)
        diff = (Y_mapped - X).abs().mean()
        
        # Should be relatively small since distributions are similar
        self.assertLess(diff.item(), 0.5)


class TestMinibatchCorrectness(TimedTestCase):
    """Test minibatch vs full-batch consistency."""

    def test_dual_objective_batch_consistency(self):
        """Test that dual objective on full batch matches expected value."""
        dim = 2
        torch.manual_seed(789)
        
        X = torch.randn(40, dim)
        Y = torch.randn(40, dim)
        
        model = FiniteModel(
            num_candidates=12,
            num_dims=dim,
            kernel=lambda x, y: L22(x, y),
            mode="concave"
        )
        
        fcot = FCOT(
            input_dim=dim,
            model=model,
            inverse_cx=inverse_grad_L22,
            inner_steps=5
        )
        
        # Full batch
        idx_full = torch.arange(len(Y))
        D_full, u_mean, uc_mean = fcot._dual_objective(X, Y, idx_full)
        
        # Should compute without error and give consistent results
        self.assertFalse(torch.isnan(D_full))

    def test_step_updates_parameters(self):
        """Test that _step actually updates model parameters."""
        dim = 2
        model = FiniteModel(
            num_candidates=8,
            num_dims=dim,
            kernel=lambda x, y: L22(x, y),
            mode="concave"
        )
        
        fcot = FCOT(
            input_dim=dim,
            model=model,
            inverse_cx=inverse_grad_L22,
            lr=1e-2
        )
        
        X = torch.randn(20, dim)
        Y = torch.randn(20, dim)
        idx_y = torch.arange(len(Y))
        
        # Get initial parameters
        initial_params = [p.clone() for p in fcot.model.parameters()]
        
        # Take a step
        stats = fcot._step(X, Y, idx_y)
        
        # Check that at least one parameter changed
        changed = False
        for init_p, curr_p in zip(initial_params, fcot.model.parameters()):
            if not torch.allclose(init_p, curr_p):
                changed = True
                break
        
        self.assertTrue(changed, "Parameters should change after optimization step")
        
        # Check that stats are returned
        self.assertIn("dual", stats)
        self.assertIn("u_mean", stats)
        self.assertIn("uc_mean", stats)


class TestEdgeCases(TimedTestCase):
    """Test edge cases and boundary conditions."""

    def test_single_point(self):
        """Test handling of single point."""
        dim = 2
        
        model = FiniteModel(
            num_candidates=5,
            num_dims=dim,
            kernel=lambda x, y: L22(x, y),
            mode="concave"
        )
        
        fcot = FCOT(
            input_dim=dim,
            model=model,
            inverse_cx=inverse_grad_L22
        )
        
        X = torch.randn(1, dim)
        Y = torch.randn(1, dim)
        idx_y = torch.tensor([0])
        
        # Should not crash
        D, u_mean, uc_mean = fcot._dual_objective(X, Y, idx_y)
        self.assertFalse(torch.isnan(D))

    def test_high_dimensional(self):
        """Test with higher dimensional data."""
        dim = 10
        torch.manual_seed(1414)
        
        X = torch.randn(30, dim)
        Y = torch.randn(30, dim)
        
        model = FiniteModel(
            num_candidates=20,
            num_dims=dim,
            kernel=lambda x, y: L22(x, y),
            mode="concave"
        )
        
        fcot = FCOT(
            input_dim=dim,
            model=model,
            inverse_cx=inverse_grad_L22,
            lr=1e-3,
            inner_steps=5
        )
        
        # Should handle high dimensions
        logs = fcot._fit(X, Y, batch_size=15, iters=10, print_every=100)
        self.assertGreater(len(logs["dual"]), 0)

    def test_zero_inner_steps_raises_or_handles(self):
        """Test behavior with zero inner steps."""
        dim = 2
        
        model = FiniteModel(
            num_candidates=5,
            num_dims=dim,
            kernel=lambda x, y: L22(x, y),
            mode="concave"
        )
        
        fcot = FCOT(
            input_dim=dim,
            model=model,
            inverse_cx=inverse_grad_L22,
            inner_steps=0  # Edge case
        )
        
        X = torch.randn(10, dim)
        Y = torch.randn(10, dim)
        
        # Should either handle gracefully or raise clear error
        try:
            logs = fcot._fit(X, Y, batch_size=5, iters=2, print_every=100)
            # If it doesn't raise, check it produces some result
            self.assertIsInstance(logs, dict)
        except (ValueError, AssertionError):
            # Acceptable to reject zero inner steps
            pass


if __name__ == "__main__":
    unittest.main()
